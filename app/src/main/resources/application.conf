# app {

  app-name = "app"
  spark.checkpoint.dir = "./tmp"

  akka {
    loglevel = "INFO"
    loggers = ["akka.event.slf4j.Slf4jLogger"]
    logging-filter = "akka.event.slf4j.Slf4jLoggingFilter"
    logger-startup-timeout = 60s
    log-dead-letters = off
    log-dead-letters-during-shutdown = off

    actor {
      provider = "akka.cluster.ClusterActorRefProvider"
      default-dispatcher {
        # Throughput for default Dispatcher, set to 1 for as fair as possible
        throughput = 10
      }
      # serializers.kryo = "com.twitter.chill.akka.AkkaSerializer"
      # serialization-bindings."scala.Product" = kryo
    }
    remote {
      log-remote-lifecycle-events = off
      netty.tcp {
        hostname = "127.0.0.1"
        port = 0
      }
    }

    cluster {
      seed-nodes = [
        "akka.tcp://ClsuterSystem@127.0.0.1:9001",
        "akka.tcp://ClsuterSystem@127.0.0.1:9002"]

      auto-down-unreachable-after = 10s
      log-info = on
      gossip-interval = 5s
      publish-stats-interval = 10s
      auto-down-unreachable-after = 10s
      metrics.gossip-interval = 10s
      metrics.collect-interval = 10s
    }
  }

  spark {
  #   # The Spark master host. Set first by environment if exists. Then system, then config.
  #   # Options: spark://host1:port1,host2:port2
  #   # - "local" to run locally with one thread,
  #   # - local[4]" to run locally with 4 cores
  #   # - the master IP/hostname to run on a Spark standalone cluster
  #   # - if not set, defaults to "local[*]" to run with enough threads
  #   # Supports optional HA failover for more than one: host1,host2..
  #   # which is used to inform: spark://host1:port1,host2:port2
    master = "local"
  #   # cleaner.ttl = ${?SPARK_CLEANER_TTL}

  #   # The batch interval must be set based on the latency requirements
  #   # of your application and available cluster resources.
  #   # streaming.batch.interval = ${?SPARK_STREAMING_BATCH_INTERVAL}
  }

# }
